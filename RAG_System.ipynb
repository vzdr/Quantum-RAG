{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System - End-to-End Retrieval-Augmented Generation\n",
    "\n",
    "This notebook implements a complete RAG pipeline with the following phases:\n",
    "\n",
    "## Phase A - Indexing (Offline)\n",
    "1. **Document Upload** - Load documents (PDF, TXT, DOCX)\n",
    "2. **Chunking** - Split documents into manageable chunks\n",
    "3. **Chunk Embedding** - Generate vector embeddings for each chunk\n",
    "4. **Vector Storage** - Store embeddings in ChromaDB\n",
    "\n",
    "## Phase B - Inference (Online)\n",
    "5. **Query Embedding** - Embed the user's query\n",
    "6. **Similarity Search** - Find similar chunks using cosine similarity\n",
    "7. **Top-K Selection** - Select the most relevant chunks\n",
    "8. **Augmented Generation** - Generate response using Gemini LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Run the cell below to install required dependencies (if not already installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to install dependencies\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Core modules\n",
    "from core import (\n",
    "    DocumentLoader, Document,\n",
    "    TextChunker, Chunk,\n",
    "    EmbeddingGenerator, EmbeddedChunk,\n",
    "    VectorStore,\n",
    "    Retriever, RetrievalResult,\n",
    "    ResponseGenerator, GenerationResult\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "from config import RAGConfig\n",
    "\n",
    "# Widgets\n",
    "from widgets import (\n",
    "    UploadWidget,\n",
    "    ChunkingWidget,\n",
    "    EmbeddingWidget,\n",
    "    QueryWidget,\n",
    "    create_embedding_visualization,\n",
    "    create_similarity_chart,\n",
    "    create_chunk_statistics_dashboard\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"GEMINI_API_KEY configured: {'Yes' if os.getenv('GEMINI_API_KEY') else 'No - Please set in .env file'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Initialize the RAG configuration with default parameters. You can modify these values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = RAGConfig(\n",
    "    # Chunking\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    chunking_strategy='sentence',\n",
    "    \n",
    "    # Embedding\n",
    "    embedding_model='all-MiniLM-L6-v2',\n",
    "    embedding_batch_size=32,\n",
    "    embedding_device='cpu',\n",
    "    \n",
    "    # Vector Store\n",
    "    collection_name='rag_demo',\n",
    "    persist_directory='./chroma_db',\n",
    "    \n",
    "    # Retrieval\n",
    "    top_k=5,\n",
    "    similarity_threshold=0.0,\n",
    "    \n",
    "    # Generation\n",
    "    llm_model='gemini-2.0-flash-lite',\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(\"Configuration initialized:\")\n",
    "for key, value in config.to_dict().items():\n",
    "    if key != 'system_prompt':\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase A: Indexing (Offline)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Document Upload\n",
    "\n",
    "Upload documents or load from the sample data directory. Supported formats: PDF, TXT, DOCX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create upload widget\n",
    "upload_widget = UploadWidget()\n",
    "upload_widget.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load sample documents programmatically\n",
    "sample_docs = DocumentLoader.load_directory('./data')\n",
    "\n",
    "for doc in sample_docs:\n",
    "    upload_widget.add_document(doc)\n",
    "    print(f\"Loaded: {doc.source} ({len(doc):,} chars)\")\n",
    "\n",
    "print(f\"\\nTotal documents: {len(upload_widget.get_documents())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loaded documents\n",
    "documents = upload_widget.get_documents()\n",
    "\n",
    "# Preview first document\n",
    "if documents:\n",
    "    print(f\"Preview of '{documents[0].source}':\\n\")\n",
    "    print(documents[0].preview(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text Chunking\n",
    "\n",
    "Split documents into smaller chunks for processing. Adjust parameters using the interactive controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunking widget\n",
    "chunking_widget = ChunkingWidget()\n",
    "chunking_widget.set_documents(documents)\n",
    "chunking_widget.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Create chunks programmatically\n",
    "chunker = TextChunker(\n",
    "    chunk_size=config.chunk_size,\n",
    "    overlap=config.chunk_overlap,\n",
    "    strategy=config.chunking_strategy\n",
    ")\n",
    "\n",
    "chunks = chunker.chunk_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "# Get statistics\n",
    "stats = chunker.get_statistics(chunks)\n",
    "print(f\"\\nStatistics:\")\n",
    "for key, value in stats.items():\n",
    "    if key != 'sources':\n",
    "        print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chunk statistics\n",
    "fig = create_chunk_statistics_dashboard(chunks)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Chunk Embedding\n",
    "\n",
    "Generate vector embeddings for each chunk using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding generator\n",
    "embedder = EmbeddingGenerator(\n",
    "    model_name=config.embedding_model,\n",
    "    device=config.embedding_device\n",
    ")\n",
    "\n",
    "print(f\"Model: {embedder.model_name}\")\n",
    "print(f\"Embedding dimension: {embedder.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks\n",
    "embedded_chunks = embedder.embed_chunks(\n",
    "    chunks,\n",
    "    batch_size=config.embedding_batch_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(embedded_chunks)} embeddings\")\n",
    "print(f\"Embedding shape: {embedded_chunks[0].embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Vector Storage\n",
    "\n",
    "Store embeddings in ChromaDB vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "vector_store = VectorStore(\n",
    "    collection_name=config.collection_name,\n",
    "    persist_directory=config.persist_directory,\n",
    "    reset=True  # Set to False to keep existing data\n",
    ")\n",
    "\n",
    "# Add embeddings\n",
    "count = vector_store.add(embedded_chunks)\n",
    "\n",
    "print(f\"Added {count} vectors to collection '{config.collection_name}'\")\n",
    "print(f\"\\nVector Store Statistics:\")\n",
    "for key, value in vector_store.get_statistics().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embedding Space\n",
    "\n",
    "Use UMAP/t-SNE/PCA to visualize the high-dimensional embeddings in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all embeddings for visualization\n",
    "all_embeddings, all_metadata = vector_store.get_all_embeddings()\n",
    "\n",
    "print(f\"Embeddings shape: {all_embeddings.shape}\")\n",
    "print(f\"Metadata count: {len(all_metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding visualization (UMAP)\n",
    "fig = create_embedding_visualization(\n",
    "    all_embeddings,\n",
    "    all_metadata,\n",
    "    method='UMAP'  # Options: 'UMAP', 't-SNE', 'PCA'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase B: Inference (Online)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 5-7: Query and Retrieval\n",
    "\n",
    "- **Step 5**: Embed the query using the same model\n",
    "- **Step 6**: Calculate cosine similarity with all chunk embeddings\n",
    "- **Step 7**: Select top-k most similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever\n",
    "retriever = Retriever(embedder, vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is machine learning and what are its main types?\"\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "results = retriever.retrieve(\n",
    "    query=query,\n",
    "    k=config.top_k,\n",
    "    threshold=config.similarity_threshold\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nRetrieved {len(results)} chunks:\")\n",
    "for r in results:\n",
    "    print(f\"\\n[Rank {r.rank}] Score: {r.score:.4f} | Source: {r.source}\")\n",
    "    print(f\"  {r.text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity scores\n",
    "fig = create_similarity_chart(results)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize query in embedding space\n",
    "query_embedding = retriever.last_query_embedding\n",
    "retrieved_indices = retriever.get_retrieved_indices(results, all_metadata)\n",
    "\n",
    "fig = create_embedding_visualization(\n",
    "    all_embeddings,\n",
    "    all_metadata,\n",
    "    method='UMAP',\n",
    "    query_embedding=query_embedding,\n",
    "    retrieved_indices=retrieved_indices\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Augmented Generation\n",
    "\n",
    "Use the retrieved context to generate a response with Gemini LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator\n",
    "generator = ResponseGenerator(\n",
    "    model=config.llm_model,\n",
    "    temperature=config.temperature,\n",
    "    max_tokens=config.max_tokens\n",
    ")\n",
    "\n",
    "print(f\"LLM Model: {generator.model_name}\")\n",
    "print(f\"Temperature: {generator.temperature}\")\n",
    "print(f\"Max Tokens: {generator.max_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "generation_result = generator.generate(\n",
    "    query=query,\n",
    "    context_chunks=results,\n",
    "    system_prompt=config.system_prompt\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GENERATED RESPONSE:\")\n",
    "print('='*60)\n",
    "print(generation_result.response)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Sources: {', '.join(generation_result.sources)}\")\n",
    "print(f\"Tokens used: {generation_result.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Interactive Query Interface\n",
    "---\n",
    "\n",
    "Use the interactive widget below to query the RAG system with your own questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive query widget\n",
    "query_widget = QueryWidget()\n",
    "query_widget.set_components(embedder, vector_store)\n",
    "query_widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Complete Pipeline Example\n",
    "---\n",
    "\n",
    "Run the entire RAG pipeline in one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_query(query: str, top_k: int = 5, temperature: float = 0.7):\n",
    "    \"\"\"\n",
    "    Run a complete RAG query.\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask\n",
    "        top_k: Number of chunks to retrieve\n",
    "        temperature: LLM temperature\n",
    "    \n",
    "    Returns:\n",
    "        GenerationResult object\n",
    "    \"\"\"\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    \n",
    "    # Step 5-7: Retrieve\n",
    "    print(\"\\n[Step 5-7] Retrieving relevant chunks...\")\n",
    "    results = retriever.retrieve(query=query, k=top_k)\n",
    "    print(f\"  Found {len(results)} relevant chunks\")\n",
    "    \n",
    "    # Show retrieved chunks\n",
    "    for r in results:\n",
    "        print(f\"    [{r.rank}] Score: {r.score:.3f} - {r.source}\")\n",
    "    \n",
    "    # Step 8: Generate\n",
    "    print(\"\\n[Step 8] Generating response...\")\n",
    "    generator.update_parameters(temperature=temperature)\n",
    "    result = generator.generate(query=query, context_chunks=results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"=\"*50)\n",
    "    print(result.response)\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(f\"Sources: {', '.join(result.sources)}\")\n",
    "    print(f\"Tokens: {result.total_tokens}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example queries\n",
    "example_queries = [\n",
    "    \"What is deep learning and how does it work?\",\n",
    "    \"How do I create a virtual environment in Python?\",\n",
    "    \"What are the ethical considerations in AI?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an example query\n",
    "result = run_rag_query(example_queries[0], top_k=5, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another query\n",
    "result = run_rag_query(example_queries[1], top_k=5, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Query\n",
    "---\n",
    "\n",
    "Enter your own query below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your custom query here\n",
    "my_query = \"What is RAG and how does it improve LLM responses?\"\n",
    "\n",
    "result = run_rag_query(my_query, top_k=5, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete RAG pipeline:\n",
    "\n",
    "1. **Document Upload** - Loaded documents from files\n",
    "2. **Chunking** - Split documents into smaller pieces\n",
    "3. **Embedding** - Generated vector representations\n",
    "4. **Storage** - Saved to ChromaDB\n",
    "5. **Query Embedding** - Converted query to vector\n",
    "6. **Similarity Search** - Found related chunks\n",
    "7. **Top-K Selection** - Selected best matches\n",
    "8. **Generation** - Created response with Gemini\n",
    "\n",
    "Each step can be customized using the interactive widgets or by modifying the configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
