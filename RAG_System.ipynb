{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System\n",
    "\n",
    "**Phase A:** Load Documents → Chunk → Embed → Store in Vector DB\n",
    "\n",
    "**Phase B:** Query → Retrieve Similar Chunks → Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Install dependencies (run once)\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Imports\n",
    "import os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from core import DocumentLoader, TextChunker, EmbeddingGenerator, VectorStore, Retriever, ResponseGenerator\n",
    "from config import RAGConfig\n",
    "from widgets import create_embedding_visualization, create_similarity_chart, create_chunk_statistics_dashboard\n",
    "\n",
    "print(\"Imports OK!\")\n",
    "print(f\"GEMINI_API_KEY: {'SET' if os.getenv('GEMINI_API_KEY') else 'NOT SET'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Configuration - EDIT THESE VALUES\n",
    "config = RAGConfig(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    chunking_strategy='sentence',\n",
    "    embedding_model='all-MiniLM-L6-v2',\n",
    "    top_k=5,\n",
    "    llm_model='gemini-2.5-flash-lite',\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024\n",
    ")\n",
    "print(\"Config set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase A: Indexing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Load Documents - EDIT PATH HERE\n",
    "\n",
    "# Option A: Sample files only\n",
    "documents = DocumentLoader.load_directory('./data')\n",
    "\n",
    "# Option B: Your own file (uncomment and edit)\n",
    "# documents = [DocumentLoader.load(r'D:\\lotr_full_text.txt')]\n",
    "\n",
    "# Option C: Both (uncomment)\n",
    "# documents = DocumentLoader.load_directory('./data')\n",
    "# documents.append(DocumentLoader.load(r'D:\\lotr_full_text.txt'))\n",
    "\n",
    "for doc in documents:\n",
    "    print(f\"{doc.source}: {len(doc):,} chars\")\n",
    "print(f\"\\nTotal: {len(documents)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Chunk documents\n",
    "chunker = TextChunker(chunk_size=config.chunk_size, overlap=config.chunk_overlap, strategy=config.chunking_strategy)\n",
    "chunks = chunker.chunk_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks (avg {chunker.get_statistics(chunks)['avg_length']:.0f} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Visualize chunks (optional)\n",
    "create_chunk_statistics_dashboard(chunks).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 7: Generate embeddings\nembedder = EmbeddingGenerator(model_name=config.embedding_model, device='cuda:0')\nembedded_chunks = embedder.embed_chunks(chunks, batch_size=32, show_progress=True)\nprint(f\"\\nGenerated {len(embedded_chunks)} embeddings\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Store in vector database\n",
    "vector_store = VectorStore(collection_name='rag_collection', persist_directory='./chroma_db', reset=True)\n",
    "count = vector_store.add(embedded_chunks)\n",
    "print(f\"Stored {count} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Visualize embedding space (optional)\n",
    "all_embeddings, all_metadata = vector_store.get_all_embeddings()\n",
    "create_embedding_visualization(all_embeddings, all_metadata, method='UMAP').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase B: Query\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Initialize retriever and generator\n",
    "retriever = Retriever(embedder, vector_store)\n",
    "generator = ResponseGenerator(model=config.llm_model, temperature=config.temperature, max_tokens=config.max_tokens)\n",
    "print(f\"Ready! Using {config.llm_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: ASK A QUESTION - EDIT YOUR QUERY HERE\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "# Retrieve\n",
    "results = retriever.retrieve(query=query, k=config.top_k)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved chunks:\")\n",
    "for r in results:\n",
    "    print(f\"  [{r.rank}] {r.score:.3f} - {r.source}\")\n",
    "\n",
    "# Generate\n",
    "response = generator.generate(query=query, context_chunks=results)\n",
    "print(f\"\\n{'='*50}\\nANSWER:\\n{'='*50}\")\n",
    "print(response.response)\n",
    "print(f\"\\nSources: {', '.join(response.sources)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 12: Visualize query in embedding space with score coloring\nquery_embedding = retriever.last_query_embedding\nretrieved_indices = retriever.get_retrieved_indices(results, all_metadata)\nscores = [r.score for r in results]\n\nfig = create_embedding_visualization(\n    all_embeddings, all_metadata, method='UMAP',\n    query_embedding=query_embedding,\n    retrieved_indices=retrieved_indices,\n    retrieval_scores=scores\n)\nfig.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: Quick ask function - use for more questions\n",
    "def ask(question, top_k=5):\n",
    "    results = retriever.retrieve(query=question, k=top_k)\n",
    "    response = generator.generate(query=question, context_chunks=results)\n",
    "    print(f\"Q: {question}\\n\\nA: {response.response}\\n\\nSources: {', '.join(response.sources)}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: Ask more questions\n",
    "ask(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"How do I create a virtual environment in Python?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}