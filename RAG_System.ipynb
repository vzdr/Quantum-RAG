{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System\n",
    "\n",
    "**Phase A:** Load Documents → Chunk → Embed → Store in Vector DB\n",
    "\n",
    "**Phase B:** Query → Retrieve Similar Chunks → Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Install dependencies (run once)\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK!\n",
      "GEMINI_API_KEY: SET\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Imports\n",
    "import os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from core import DocumentLoader, TextChunker, EmbeddingGenerator, VectorStore, Retriever, ResponseGenerator\n",
    "from config import RAGConfig\n",
    "from widgets import create_embedding_visualization, create_similarity_chart, create_chunk_statistics_dashboard\n",
    "\n",
    "print(\"Imports OK!\")\n",
    "print(f\"GEMINI_API_KEY: {'SET' if os.getenv('GEMINI_API_KEY') else 'NOT SET'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set!\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Configuration - EDIT THESE VALUES\n",
    "config = RAGConfig(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    chunking_strategy='sentence',\n",
    "    embedding_model='all-MiniLM-L6-v2',\n",
    "    top_k=5,\n",
    "    llm_model='gemini-2.5-flash-lite',\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024\n",
    ")\n",
    "print(\"Config set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase A: Indexing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lotr_full_text.txt: 2,579,963 chars\n",
      "\n",
      "Total: 1 docs\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Load Documents - EDIT PATH HERE\n",
    "documents = DocumentLoader.load_directory('./data/lotr')\n",
    "\n",
    "for doc in documents:\n",
    "    print(f\"{doc.source}: {len(doc):,} chars\")\n",
    "print(f\"\\nTotal: {len(documents)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6050 chunks (avg 432 chars)\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Chunk documents\n",
    "chunker = TextChunker(chunk_size=config.chunk_size, overlap=config.chunk_overlap, strategy=config.chunking_strategy)\n",
    "chunks = chunker.chunk_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks (avg {chunker.get_statistics(chunks)['avg_length']:.0f} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 6: Visualize chunks (optional - comment out to skip)\n# create_chunk_statistics_dashboard(chunks).show()\nprint(f\"Chunk statistics: {len(chunks)} chunks created\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3439369511549e697b40df1a21f8e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding chunks:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Model loaded (dim=384)\n",
      "\n",
      "Generated 6050 embeddings\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Generate embeddings\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "embedder = EmbeddingGenerator(model_name=config.embedding_model, device=device)\n",
    "embedded_chunks = embedder.embed_chunks(chunks, batch_size=32, show_progress=True)\n",
    "print(f\"\\nGenerated {len(embedded_chunks)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: rag_collection\n",
      "Stored 6050 vectors\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: Store in vector database\n",
    "vector_store = VectorStore(collection_name='rag_collection', persist_directory='./chroma_db', reset=True)\n",
    "count = vector_store.add(embedded_chunks)\n",
    "print(f\"Stored {count} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 9: Visualize embedding space (OPTIMIZED - now much faster!)\nall_embeddings, all_metadata = vector_store.get_all_embeddings()\n\n# Automatic subsampling to 2000 points + faster UMAP params\n# Should take ~5-10s instead of 40s+\ncreate_embedding_visualization(all_embeddings, all_metadata, method='UMAP').show()\n\nprint(f\"Visualized {len(all_embeddings)} embeddings (subsampled to 2000 for speed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase B: Query\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready! Using gemini-2.5-flash-lite\n"
     ]
    }
   ],
   "source": [
    "# CELL 10: Initialize retriever and generator\n",
    "retriever = Retriever(embedder, vector_store)\n",
    "generator = ResponseGenerator(model=config.llm_model, temperature=config.temperature, max_tokens=config.max_tokens)\n",
    "print(f\"Ready! Using {config.llm_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is machine learning?\n",
      "\n",
      "Retrieved chunks:\n",
      "\n",
      "================================================================================\n",
      "[1] Score: 0.142 | Source: lotr_full_text.txt\n",
      "================================================================================\n",
      "Frodo?' he said. 'What's the time? Seems to be getting late!'\n",
      "     'No it isn't,' said Frodo. `But the day is getting darker instead of lighter: darker and darker. As far as I can tell, it isn't midday yet, and you've only slept for about three hours.'\n",
      "     'I wonder what's up,' said Sam. 'Is there a storm coming? If so it's going to be the worst there ever was. We shall wish we were down a deep hole, not just stuck under a hedge.' He listened. `What's that? Thunder, or drums, or what is it?\n",
      "\n",
      "================================================================================\n",
      "[2] Score: 0.136 | Source: lotr_full_text.txt\n",
      "================================================================================\n",
      "Speak, friend, and enter_. And underneath small and faint is written: _I, Narvi, made them. Celebrimbor of Hollin drew these signs._'\n",
      "     `What does it mean by _speak, friend, and enter_?' asked Merry. 'That is plain enough,' said Gimli. `If you are a friend, speak the password, and the doors will open, and you can enter.'\n",
      "     'Yes,' said Gandalf, 'these doors are probably governed by words.\n",
      "\n",
      "================================================================================\n",
      "[3] Score: 0.131 | Source: lotr_full_text.txt\n",
      "================================================================================\n",
      "I don't know but it felt as if something that grew in the ground-asleep, you might say, or just feeling itself as something between roof-tip and leaf-tip, between deep earth and sky had suddenly waked up, and was considering you with the same slow care that it had given to its own inside affairs for endless years.'\n",
      "     '_Hrum, Hoom_,' murmured the voice, a deep voice like a very deep woodwind instrument. 'Very odd indeed! Do not be hasty, that is my motto.\n",
      "\n",
      "================================================================================\n",
      "[4] Score: 0.122 | Source: lotr_full_text.txt\n",
      "================================================================================\n",
      "Then rising swiftly up, far above the Towers of the Black Gate, high above the mountains, a vast soaring darkness sprang into the sky, flickering with fire. The earth groaned and quaked. The Towers of the Teeth swayed, tottered, and fell down; the mighty rampart crumbled; the Black Gate was hurled in ruin; and from far away, now dim, now growing, now mounting to the clouds, there came a drumming rumble, a roar, a long echoing roll of ruinous noise. 'The realm of Sauron is ended!' said Gandalf.\n",
      "\n",
      "================================================================================\n",
      "[5] Score: 0.121 | Source: lotr_full_text.txt\n",
      "================================================================================\n",
      "But before Sam could make up his mind what it was that he saw, the light faded; and now he thought he saw Frodo with a pale face lying fast asleep under a great dark cliff. Then he seemed to see himself going along a dim passage, and climbing an endless winding stair. It came to him suddenly that he was looking urgently for something, but what it was he did not know. Like a dream the vision shifted and went back, and he saw the trees again.\n",
      "\n",
      "==================================================\n",
      "ANSWER:\n",
      "==================================================\n",
      "I cannot find this information in the provided documents.\n",
      "\n",
      "Sources: lotr_full_text.txt\n"
     ]
    }
   ],
   "source": [
    "# CELL 11: ASK A QUESTION - EDIT YOUR QUERY HERE\n",
    "query = \"What is a potato\"\n",
    "\n",
    "# Retrieve\n",
    "results = retriever.retrieve(query=query, k=config.top_k)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved chunks:\")\n",
    "for r in results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{r.rank}] Score: {r.score:.3f} | Source: {r.source}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(r.text)\n",
    "\n",
    "# Generate\n",
    "response = generator.generate(query=query, context_chunks=results)\n",
    "print(f\"\\n{'='*50}\\nANSWER:\\n{'='*50}\")\n",
    "print(response.response)\n",
    "print(f\"\\nSources: {', '.join(response.sources)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 12: Visualize query in embedding space (optional - comment out if slow)\n# query_embedding = retriever.last_query_embedding\n# retrieved_indices = retriever.get_retrieved_indices(results, all_metadata)\n# scores = [r.score for r in results]\n# \n# fig = create_embedding_visualization(\n#     all_embeddings, all_metadata, method='UMAP',\n#     query_embedding=query_embedding,\n#     retrieved_indices=retrieved_indices,\n#     retrieval_scores=scores\n# )\n# fig.show()\n\nprint(\"Query visualization skipped (uncomment above to enable)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: Quick ask function - use for more questions\n",
    "def ask(question, top_k=5):\n",
    "    results = retriever.retrieve(query=question, k=top_k)\n",
    "    response = generator.generate(query=question, context_chunks=results)\n",
    "    print(f\"Q: {question}\\n\\nA: {response.response}\\n\\nSources: {', '.join(response.sources)}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: Ask more questions\n",
    "ask(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"How do I create a virtual environment in Python?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Diverse Retrieval: MMR vs QUBO\n",
    "Compare retrieval methods for better diversity\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import diversity modules\n",
    "from core.retrieval_strategies import create_retrieval_strategy\n",
    "from core.diversity_metrics import compare_retrieval_methods, print_comparison_table\n",
    "import numpy as np\n",
    "\n",
    "print(\"Diversity modules loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Naive vs MMR vs QUBO\n",
    "def compare_diverse(query, k=5):\n",
    "    \"\"\"Compare retrieval methods on a query.\"\"\"\n",
    "    query_emb = embedder.embed_query(query)\n",
    "    candidates = vector_store.search(query_emb, k=k*3)\n",
    "    \n",
    "    # Strategies\n",
    "    strategies = {\n",
    "        'Naive': create_retrieval_strategy('naive'),\n",
    "        'MMR': create_retrieval_strategy('mmr', lambda_param=0.5),\n",
    "        'QUBO': create_retrieval_strategy('qubo', alpha=0.6, \n",
    "                                          solver_params={'n_replicas': 2, 'full_sweeps': 5000})\n",
    "    }\n",
    "    \n",
    "    results_dict = {}\n",
    "    all_results = {}\n",
    "    \n",
    "    for name, strategy in strategies.items():\n",
    "        print(f\"\\n{name}...\", end=' ')\n",
    "        results, metadata = strategy.retrieve(query_emb, candidates, k=k)\n",
    "        all_results[name] = results\n",
    "        \n",
    "        # Convert for metrics\n",
    "        results_dict[name] = [{\n",
    "            'id': r.id,\n",
    "            'score': r.score,\n",
    "            'embedding': next((c['embedding'] for c in candidates if c['id'] == r.id), None)\n",
    "        } for r in results]\n",
    "        \n",
    "        print(f\"{metadata.get('execution_time', 0):.2f}s\")\n",
    "    \n",
    "    # Show metrics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    comparison = compare_retrieval_methods(results_dict)\n",
    "    print_comparison_table(comparison)\n",
    "    \n",
    "    return all_results, comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "query = \"Who is Frodo and what is his quest?\"\n",
    "results, comparison = compare_diverse(query, k=5)\n",
    "\n",
    "# Show results from each method\n",
    "for method_name in ['Naive', 'MMR', 'QUBO']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{method_name} Results:\")\n",
    "    print('='*70)\n",
    "    for r in results[method_name][:3]:  # Show top 3\n",
    "        print(f\"[{r.rank}] {r.score:.3f} | {r.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer using QUBO results\n",
    "response = generator.generate(query=query, context_chunks=results['QUBO'])\n",
    "print(\"=\"*70)\n",
    "print(\"QUBO-RAG ANSWER:\")\n",
    "print(\"=\"*70)\n",
    "print(response.response)\n",
    "print(f\"\\nSources: {', '.join(response.sources)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quantum-RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}