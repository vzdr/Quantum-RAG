{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum-RAG Demo: Diversity-Aware Retrieval\n",
    "\n",
    "This demo compares four retrieval strategies:\n",
    "- **Top-K**: Traditional cosine similarity ranking\n",
    "- **MMR**: Maximal Marginal Relevance (balances relevance and diversity)\n",
    "- **QUBO (Gurobi)**: Quantum-inspired QUBO optimization using classical solver\n",
    "- **QUBO (ORBIT)**: Quantum-inspired QUBO optimization using p-bit simulator\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from core.utils import (\n",
    "    load_wikipedia_dataset,\n",
    "    filter_chunks_by_prompt,\n",
    "    get_prompt_embedding,\n",
    "    compute_aspect_recall\n",
    ")\n",
    "from core.retrieval import NaiveRetrieval, MMRRetrieval, QUBORetrieval\n",
    "from core.data_models import RetrievalResult\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wikipedia dataset\n",
    "chunks, embeddings = load_wikipedia_dataset('./data/wikipedia')\n",
    "print(f\"Loaded {len(chunks):,} chunks with {len(embeddings):,} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first available prompt\n",
    "all_prompts = [c for c in chunks if c.get('chunk_type') == 'prompt']\n",
    "TEST_PROMPT_ID = all_prompts[0]['prompt_id']\n",
    "prompt_text = all_prompts[0]['text']\n",
    "query_embedding = get_prompt_embedding(chunks, embeddings, TEST_PROMPT_ID)\n",
    "\n",
    "print(f\"Test Prompt: {prompt_text[:150]}...\")\n",
    "print(f\"\\nQuery embedding shape: {query_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Case\n",
    "\n",
    "We'll test at **redundancy level 5** (high redundancy) where each gold aspect has 5 duplicate copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get candidates at redundancy level 5\n",
    "redundancy_level = 5\n",
    "k = 5\n",
    "candidates, gold_aspects, _, _, _ = filter_chunks_by_prompt(chunks, TEST_PROMPT_ID, redundancy_level)\n",
    "\n",
    "# Prepare candidate data structure\n",
    "candidate_results = [{\n",
    "    'id': cand['chunk_id'],\n",
    "    'text': cand['text'],\n",
    "    'embedding': embeddings.get(cand['chunk_id']),\n",
    "    'score': np.dot(\n",
    "        query_embedding / np.linalg.norm(query_embedding),\n",
    "        embeddings.get(cand['chunk_id']) / np.linalg.norm(embeddings.get(cand['chunk_id']))\n",
    "    ),\n",
    "    'metadata': cand\n",
    "} for cand in candidates if embeddings.get(cand['chunk_id']) is not None]\n",
    "\n",
    "candidate_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "print(f\"Redundancy Level: {redundancy_level}\")\n",
    "print(f\"Candidate pool: {len(candidate_results)} chunks\")\n",
    "print(f\"Gold aspects: {len(gold_aspects)}\")\n",
    "print(f\"Retrieving k={k} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Top-K (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K retrieval\n",
    "topk_strategy = NaiveRetrieval()\n",
    "topk_results = topk_strategy.retrieve(query_embedding, candidate_results, k)\n",
    "topk_meta = [r.chunk.metadata for r in topk_results]\n",
    "topk_recall, topk_count = compute_aspect_recall(topk_meta, gold_aspects)\n",
    "\n",
    "print(\"Top-K Results:\")\n",
    "print(f\"  Aspect Recall: {topk_recall:.1f}% ({topk_count}/{len(gold_aspects)} aspects)\")\n",
    "print(f\"\\nSelected chunks:\")\n",
    "for i, r in enumerate(topk_results, 1):\n",
    "    chunk_type = r.chunk.metadata.get('chunk_type', 'unknown')\n",
    "    aspect = r.chunk.metadata.get('aspect_index', -1)\n",
    "    print(f\"  [{i}] Aspect {aspect} | {chunk_type} | {r.chunk.text[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: MMR (Maximal Marginal Relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR retrieval\n",
    "lambda_param = 0.5  # Balance between relevance (1.0) and diversity (0.0)\n",
    "mmr_strategy = MMRRetrieval(lambda_param=lambda_param)\n",
    "mmr_results = mmr_strategy.retrieve(query_embedding, candidate_results, k)\n",
    "mmr_meta = [r.chunk.metadata for r in mmr_results]\n",
    "mmr_recall, mmr_count = compute_aspect_recall(mmr_meta, gold_aspects)\n",
    "\n",
    "print(f\"MMR Results (lambda={lambda_param}):\")\n",
    "print(f\"  Aspect Recall: {mmr_recall:.1f}% ({mmr_count}/{len(gold_aspects)} aspects)\")\n",
    "print(f\"\\nSelected chunks:\")\n",
    "for i, r in enumerate(mmr_results, 1):\n",
    "    chunk_type = r.chunk.metadata.get('chunk_type', 'unknown')\n",
    "    aspect = r.chunk.metadata.get('aspect_index', -1)\n",
    "    print(f\"  [{i}] Aspect {aspect} | {chunk_type} | {r.chunk.text[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: QUBO with Gurobi (Classical Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUBO with Gurobi\n",
    "alpha = 0.04\n",
    "penalty = 10.0\n",
    "beta = 0.8\n",
    "\n",
    "gurobi_strategy = QUBORetrieval(alpha=alpha, penalty=penalty, beta=beta, solver='gurobi')\n",
    "gurobi_results = gurobi_strategy.retrieve(query_embedding, candidate_results, k)\n",
    "gurobi_meta = [r.chunk.metadata for r in gurobi_results]\n",
    "gurobi_recall, gurobi_count = compute_aspect_recall(gurobi_meta, gold_aspects)\n",
    "\n",
    "print(f\"QUBO-Gurobi Results (alpha={alpha}, penalty={penalty}, beta={beta}):\")\n",
    "print(f\"  Aspect Recall: {gurobi_recall:.1f}% ({gurobi_count}/{len(gold_aspects)} aspects)\")\n",
    "print(f\"\\nSelected chunks:\")\n",
    "for i, r in enumerate(gurobi_results, 1):\n",
    "    chunk_type = r.chunk.metadata.get('chunk_type', 'unknown')\n",
    "    aspect = r.chunk.metadata.get('aspect_index', -1)\n",
    "    print(f\"  [{i}] Aspect {aspect} | {chunk_type} | {r.chunk.text[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: QUBO with ORBIT (P-bit Simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUBO with ORBIT\n",
    "try:\n",
    "    orbit_strategy = QUBORetrieval(alpha=alpha, penalty=penalty, beta=beta, solver='orbit')\n",
    "    orbit_results = orbit_strategy.retrieve(query_embedding, candidate_results, k)\n",
    "    orbit_meta = [r.chunk.metadata for r in orbit_results]\n",
    "    orbit_recall, orbit_count = compute_aspect_recall(orbit_meta, gold_aspects)\n",
    "\n",
    "    print(f\"QUBO-ORBIT Results (alpha={alpha}, penalty={penalty}, beta={beta}):\")\n",
    "    print(f\"  Aspect Recall: {orbit_recall:.1f}% ({orbit_count}/{len(gold_aspects)} aspects)\")\n",
    "    print(f\"\\nSelected chunks:\")\n",
    "    for i, r in enumerate(orbit_results, 1):\n",
    "        chunk_type = r.chunk.metadata.get('chunk_type', 'unknown')\n",
    "        aspect = r.chunk.metadata.get('aspect_index', -1)\n",
    "        print(f\"  [{i}] Aspect {aspect} | {chunk_type} | {r.chunk.text[:60]}...\")\n",
    "except ImportError:\n",
    "    print(\"ORBIT not available. Install with: cd orbit && uv pip install orbit-0.2.0-py3-none-any.whl\")\n",
    "    orbit_recall = None\n",
    "    orbit_count = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: All Methods at Redundancy Level 5\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<20} {'Aspect Recall':<20} {'Aspects Found'}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Top-K':<20} {topk_recall:>6.1f}% {f'({topk_count}/{len(gold_aspects)})':>20}\")\n",
    "print(f\"{'MMR':<20} {mmr_recall:>6.1f}% {f'({mmr_count}/{len(gold_aspects)})':>20}\")\n",
    "print(f\"{'QUBO (Gurobi)':<20} {gurobi_recall:>6.1f}% {f'({gurobi_count}/{len(gold_aspects)})':>20}\")\n",
    "if orbit_recall is not None:\n",
    "    print(f\"{'QUBO (ORBIT)':<20} {orbit_recall:>6.1f}% {f'({orbit_count}/{len(gold_aspects)})':>20}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Highlight best method\n",
    "methods = {'Top-K': topk_recall, 'MMR': mmr_recall, 'QUBO (Gurobi)': gurobi_recall}\n",
    "if orbit_recall is not None:\n",
    "    methods['QUBO (ORBIT)'] = orbit_recall\n",
    "best_method = max(methods, key=methods.get)\n",
    "print(f\"\\nBest Method: {best_method} with {methods[best_method]:.1f}% aspect recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "method_names = ['Top-K', 'MMR', 'QUBO\\n(Gurobi)']\n",
    "recalls = [topk_recall, mmr_recall, gurobi_recall]\n",
    "colors = ['#e74c3c', '#f39c12', '#3498db']\n",
    "\n",
    "if orbit_recall is not None:\n",
    "    method_names.append('QUBO\\n(ORBIT)')\n",
    "    recalls.append(orbit_recall)\n",
    "    colors.append('#9b59b6')\n",
    "\n",
    "bars = ax.bar(method_names, recalls, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, recall in zip(bars, recalls):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'{recall:.1f}%',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('Aspect Recall (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Retrieval Method Comparison (Redundancy Level {redundancy_level})',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylim(0, 105)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.axhline(y=100, color='green', linestyle='--', alpha=0.5, linewidth=1.5, label='Perfect (100%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Top-K** struggles with redundancy - selects duplicate chunks of the same aspect\n",
    "2. **MMR** improves diversity by penalizing similarity to already-selected chunks\n",
    "3. **QUBO (Gurobi)** uses classical optimization to find optimal relevance-diversity tradeoff\n",
    "4. **QUBO (ORBIT)** uses quantum-inspired p-bit computing for the same optimization\n",
    "\n",
    "Both QUBO methods balance:\n",
    "- **Relevance**: Maximize similarity to query\n",
    "- **Diversity**: Minimize similarity among selected chunks\n",
    "- **Cardinality**: Ensure exactly k chunks are selected\n",
    "\n",
    "The QUBO energy function:\n",
    "$$E(x) = -\\mathbf{r}^T\\mathbf{x} + \\alpha \\mathbf{x}^T \\mathbf{Q} \\mathbf{x} + P(\\sum x_i - k)^2$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{r}$ = relevance scores\n",
    "- $\\mathbf{Q}$ = pairwise similarity matrix (thresholded)\n",
    "- $\\alpha$ = diversity weight (0.04)\n",
    "- $P$ = cardinality penalty (10.0)\n",
    "- $k$ = target number of selections (5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
